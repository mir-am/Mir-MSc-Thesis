% Summary of my Master's thesis in one page

\documentclass[a4paper, 12pt]{article}
\thispagestyle{empty}
%opening
%\title{}
%\author{}

\usepackage[top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage{newtxtt} % Times new roman font

\begin{document}
	
\begin{center}
	{\Large \textbf{Twin Support Vector Machine for Noisy Data}} \\[0.5cm]
	
	A thesis submitted for the degree of Master of Science in Computer Engineering: Artificial Intelligence \\[0.25cm]
	
	by \\[0.25cm]
	{\large Amir M. Mir} \\ [0.5cm]
	
	Thesis Advisor: {Dr. Jalal A. Nasiri} \\[0.25cm]
	
	ISLAMIC AZAD UNIVERSITY \\
	North Tehran Branch \\[0.25cm]
	
	January, 2019
	
\end{center}

\large{\textbf{Summary}}\\[0.25cm]

In the past decade, machine learning algorithm have been used for solving problems with complex patterns. Classification is the widely-used learning method, which solves complex learning problems such as face recognition, text classification and medical diagnosis.

Support Vector Machine (SVM) is a powerful classification method. It is on the basis of Structural Risk Minimization (SRM) and VC-dimension. Due to the SRM principle, SVM has a great generalization ability. Therefore, it has been applied to a wide variety of applications. The main idea of the SVM is to find the optimal separating hyperplane with largest margin between the two classes. To obtain such a hyperplane, SVM solves a convex Quadratic Programming Problem (QPP). 

On the basis of standard SVM, scholars have proposed new classifiers in the past decade. For instance, twin support vector machine (TSVM) was proposed with the aim of classification using two non-parallel hyperplanes. Each of which fits the samples of its own class and is far from the samples of other class. Unlike SVM, TSVM solves two smaller-sized QPPs which makes its learning speed 4 times faster than that of SVM. Although TSVM has better prediction accuracy and time complexity, it has several important drawbacks: (1) Its classification performance is sensitive to noise and outliers. (2) Because of Empirical Risk Minimization (ERM), overfitting may occur. (3) Its overall computational complexity is high for large-scale datasets.



\end{document}