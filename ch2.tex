% Chapter 2 Support Vector Machine

\chapter{پشینیه پژوهش} \label{ch:2}
\section{ماشین بردار پشتیبان} \label{sec:2:1}
در این بخش، ابتدا ماشین بردار پشتیبان خطی و غیر خطی شرح داده می‌شود. سپس پیشینه پژوهش در زمینه ماشین بردار پشتیبان بررسی می‌گردد.
\subsection{ماشین بردار پشتیبان با حاشیه سخت} \label{sec:2:1:1}
ماشین بردار پشتیبان با هدف جداسازی نمونه‌های دو کلاس در سال 1995 معرفی گردید \cite{vapnik1995}. ایده اصلی این روش یادگیری، بدست آوردن ابرصفحه بهینه‌ای است که از نمونه‌های دو کلاس تا جای ممکن بیشترین فاصله را داشته باشد. به عبارت دیگر، این روش یادگیری حاشیه دو کلاس را بیشینه می‌کند. برای فهم بهتر ایده این روش، فرض کنید مجموعه داده‌ی $T=\{(x_1, y_1),(x_2, y_2) \cdots , (x_m, y_m)\}$ را در اختیار داریم که شامل $m$ تا نمونه آموزشی است. هر نمونه آموزشی $x_{i} \in \mathbb{R}^{n}$ با $n$ ویژگی در فضای ورودی و $y_i \in \{-1,1\}$ برچسب نمونه‌ی $x_i$ می‌باشد. در ساده‌ترین حالت، نمونه‌های دو کلاس در مجموعه داده $T$ با یک ابرصفحه $w^{T}x+b$ بدون خطا دسته‌بندی می‌شود. این حالت مسئله حاشیه سخت\footnote{\lr{Hard margin}}  نامیده می‌شود. شکل \ref{fig:SVM-HM} مسئله حاشیه سخت در روش \lr{SVM} را نشان می‌دهد. (لازم به ذکر است، خطوط نقطه‌چین در شکل ‏\ref{fig:SVM-HM} نشان دهنده حاشیه است.) در این حالت، یک مسئله بهینه‌سازی برای بدست آوردن ابرصفحه باید حل گردد.

\begin{equation}
\begin{gathered} 
\mathop{min}\limits_{w}\frac{1}{2}{{\left\| w \right\|}^{2}} \\
\textrm{\lr{s.t. }} {{y}_{i}}({{w}^T}{{x}_{i}}+b)\ge 1,\forall i
\end{gathered}
\label{eq:1}
\end{equation}

\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.5]{SVM-HardMargin}
	\caption{مسئله حاشیه سخت در ماشین بردار پشتیبان}
	\label{fig:SVM-HM}
\end{figure}

\noindent در رابطه \ref{eq:1}، بردار $w$ مختصات ابرصفحه و $b$ بایاس است. قید این مسئله بهنیه‌سازی بیان می‌کند که تمام نمونه‌های آموزشی باید از ابرصفحه به مقدار 1 یا بیشتر فاصله داشته باشند. به عبارت دیگر، تمام نمونه‌های آموزشی باید روی خط حاشیه یا قبل از آن قرار بگیرند. برای حل کردن مسئله بهینه‌سازی \ref{eq:1} از تابع لاگرانژ\footnote{\lr{Lagrange}}  استفاده می‌شود که در رابطه زیر تعریف شده است. 
\begin{equation}
L(w,b,\alpha )=\frac{1}{2}{{\left\| w \right\|}^{2}}-\sum\limits_{i=1}^{m}{{{\alpha }_{i}}({{y}_{i}}({{w}^{T}}{{x}_{i}}+b)-1})
\label{eq:2}
\end{equation} 
در رابطه \ref{eq:2}، بردار $\alpha$ نشان دهنده ضرایب لاگرانژ است. برای حل کردن رابطه \ref{eq:2}، از تابع لاگرانژ نسبت به $w$ و  $b$ مشتق می‌گیریم.
\begin{align}
\label{eq:3}
\begin{split}
\frac{\partial L}{\partial b}=0&\quad \Rightarrow \quad \sum\limits_{i=1}^{m}{{{\alpha }_{i}}{{y}_{i}}}=0 
\end{split}\\ 
\label{eq:4}
\begin{split}
\frac{\partial L}{\partial w}=0& \quad \Rightarrow \quad w = \sum\limits_{i=1}^{m}{\alpha_{i}y_{i}x_{i}}. 
\end{split} 
\end{align}

با استفاده از اصل دوگانگی لاگرانژ، مسئله اصلی\footnote{\lr{Primal problem}}  در رابطه \ref{eq:1} می‌تواند به مسئله دوگان\footnote{\lr{Dual problem}}  تبدیل شود که حل کردن آن آسان‌تر از مسئله اصلی خواهد بود. با در نظر گرفتن روابط \ref{eq:3} و \ref{eq:4}، مسئله دوگان برای حالت حاشیه سخت به صورت زیر تعریف می‌شود.
\begin{equation}
\begin{split} 
\min\limits_{\alpha} \quad &\frac{1}{2}\sum\limits_{i=1}^{m}{\sum\limits_{j=1}^{m}{{{\alpha }_{i}}{{\alpha }_{j}}{{y}_{i}}{{y}_{j}}x_{i}^{T}{{x}_{j}}-\sum\limits_{k=1}^{m}{{{\alpha }_{k}}}}} \\
\textrm{\lr{s.t. }} \quad &\sum\limits_{i=1}^{m}{\alpha_{i}y_{i}}=0, \\
&\alpha_{i}\ge 0,\text{ }\forall i
\end{split}
\label{eq:5}
\end{equation}
بعد از حل کردن مسئله دوگان \ref{eq:5}، اکثر مقادیر بردار $\alpha$ برابر با صفر خواهد. با این حال ضرایب لاگرانژ  $\alpha_i$ متناظر با نمونه‌های $x_i$ بزرگتر از صفر خواهد بود، اگر معادله زیر برقرار باشد.
\begin{equation}
{{y}_{i}}({{w}^{T}}{{x}_{i}}+b)=1
\label{eq:6}
\end{equation}
\indent نمونه‌های  که رابطه \ref{eq:6} برای آن‌ها برقرار باشد، به اصطلاح بردار پشتیبان\footnote{\lr{Suppor Vector (SV)}}  نامیده می‌شود. ضرایب لاگرانژ متناظر با بردار‌های پشتیبان بزرگ‌تر از صفر است. همچنین این بردارها روی حاشیه قرار می‌گیرند. بردار $w$  و بایاس  $b$ از طریق رابطه  بدست می‌آید.
\begin{equation}
\begin{split}
w &= \sum\limits_{i=1}^{m}{\alpha_{i}y_{i}x_{i}} \\
b &= y_{i}-{w}^{T}{x}_{i}
\end{split}
\label{eq:7}
\end{equation}
یک نمونه جدید یا تست بر اساس تابع تصمیم در رابطه زیر به یکی از کلاس‌های 1 و 1- تعلق می‌گیرد.
\begin{equation}
D(x)=sign({{w}^{T}}x+b) = sign(\sum\limits_{i=1}^{m}{\alpha_{i}y_{i}x_{i}^{T}} x + b)
\label{eq:8}
\end{equation}
\indent در مسئله حاشیه سخت، فرض گرفته می‌شود که تمام نمونه‌های دو کلاس به صورت خطی جدا پذیر هستند. با این حال در دنیای واقعی داده‌ها اغلب به صورت خطی جدا پذیر نیستند. در حالتی که داده‌ها به صورتی خطی جداپذیر نیستند، مسئله حاشیه نرم\footnote{\lr{Soft margin}}  در ماشین بردار پشتیبان مطرح شده است.

\subsection{ماشین بردار پشتیبان با حاشیه نرم} \label{sec:2:1:2}
در این حالت اجازه خطا در دسته‌بندی نمونه‌ها داده می‌شود. به عبارت دیگر، با معرفی متغیر کمکی\footnote{\lr{Slack variable}} $\xi$ ، تاثیر بعضی از نمونه‌های آموزشی در ایجاد حاشیه و ابرصفحه کم می‌شود. برای درک بهتر، مسئله حاشیه نرم در ماشین بردار پشتیبان در شکل \ref{fig:SVM-SM}‏ نشان داده شده است. در حالت حاشیه نرم، برای بدست آوردن ابرصفحه مسئله بهینه‌سازی به صورت زیر تعریف می‌شود.

\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.5]{SVM-SoftMargin}
	\caption{مسئله حاشیه نرم در ماشین بردار پشتیبان}
	\label{fig:SVM-SM}
\end{figure}

\begin{equation}
\begin{split}
\min_{w} \quad &\frac{1}{2}{{\left\| w \right\|}^{2}}+C\sum\limits_{i=1}^{n}{{{\xi }_{i}}} \\
\textrm{\lr{s.t. }}  \quad &{{y}_{i}}({{w}^{T}}{{x}_{i}}+b)\text{ }+{{\xi }_{i}}\ge 1,\forall i
\end{split}
\label{eq:9}
\end{equation}
در رابطه \ref{eq:9}، متغیر $\xi _{i}\ge 0$ خطای متناظر برای هر نمونه  $x_i$ است. پارامتر $C$ بیانگر تعادل بین بیشینه کردن حاشیه و خطای دسته‌بندی است. مقدار این پارامتر قبل از آموزش دسته‌بند باید مشخص گردد. مانند مسئله حاشیه سخت، برای حل کردن مسئله اصلی  از تابع لاگرانژ استفاده می‌شود که در رابطه زیر تعریف شده است.
\begin{equation}
L(w,b,\xi, \alpha, \beta )=\frac{1}{2}{{\left\| w \right\|}^{2}} + C\sum\limits_{i=1}^{m}{{{\xi }_{i}}}  -\sum\limits_{i=1}^{m}{{{\alpha }_{i}}({{y}_{i}}({{w}^{T}}{{x}_{i}}+b)-1 + \xi_{i} }) - \sum\limits_{i=1}^{m}{\beta_{i}\xi_{i}}
\label{eq:10}
\end{equation}
در رابطه \ref{eq:10}، بردار $\alpha$  و $\beta$  ضرایب لاگرانژ هستند. برای حل کردن رابطه \ref{eq:10}، نسبت به  $w$، $b$ و $\xi$  مشتق می‌گیریم.
\begin{align}
\label{eq:11}
\begin{split}
\frac{\partial L}{\partial b}&=0\quad \Rightarrow \quad \sum\limits_{i=1}^{m}{{{\alpha }_{i}}{{y}_{i}}}=0
\end{split}\\ 
\label{eq:12}
\begin{split}
\frac{\partial L}{\partial w}&=0 \quad \Rightarrow \quad w = \sum\limits_{i=1}^{m}{\alpha_{i}y_{i}x_{i}}.
\end{split}\\
\label{eq:13}
\begin{split}
\frac{\partial L}{\partial \xi}&=0 \quad \Rightarrow \quad \alpha_{i} + \beta_{i} = C
\end{split}  
\end{align}
با در نظر گرفتن روابط بالا، مسئله دوگان برای حالت حاشیه نرم به صورت زیر تعریف می‌شود.
\begin{equation}
\begin{split} 
\min\limits_{\alpha} \quad &\frac{1}{2}\sum\limits_{i=1}^{m}{\sum\limits_{j=1}^{m}{{{\alpha }_{i}}{{\alpha }_{j}}{{y}_{i}}{{y}_{j}}x_{i}^{T}{{x}_{j}}-\sum\limits_{k=1}^{m}{{{\alpha }_{k}}}}} \\
\textrm{\lr{s.t. }} \quad &\sum\limits_{i=1}^{m}{\alpha_{i}y_{i}}=0, \\
&0 \le \alpha_{i}\le C,\text{ }\forall i
\end{split}
\label{eq:14}
\end{equation}
\indent راه حل مسئله \ref{eq:14} شبیه به مسئله \ref{eq:5} در حالت حاشیه سخت است. با این تفاوت که برای ضرایب لاگرانژ یک حد بین صفر تا پارامتر  تعریف شده است. پارامتر  انعطاف‌پذیری دسته‌بند را افزایش می‌دهد. مقدار این پارامتر با داشتن دانش قبلی از مسئله و یا جستجوی شبکه‌ای تعیین می‌شود. همانطور که در شکل ‏\ref{fig:SVM-SM} نشان داده شده است، بردارهای پشتیبان لزوما روی خط حاشیه قرار ندارند. در مسئله حاشیه نرم، تابع تصمیم مشابه حالت حاشیه سخت به صورت زیر تعریف می‌شود.
\begin{equation}
D(x)=sign({{w}^{T}}x+b) = sign(\sum\limits_{i \in S}{\alpha_{i}y_{i}x_{i}^{T}} x + b)
\label{eq:15}
\end{equation}
در رابطه \ref{eq:15}، مجموعه $S$ نشان دهنده بردارهای پشتیبان است.

\subsection{ماشین بردار پشتیبان با هسته غیر خطی}\label{sec:2:1:3}
در مسائل حاشیه سخت و نرم، فرض گرفته می‌شود که نمونه‌ها در فضای ورودی به صورت خطی جدا پذیر هستند. با این حال در مواردی که نمونه‌ها به صورت خطی جدا پذیر نیستند، نمونه‌ها $x_i$ به فضای ویژگی\footnote{\lr{Feature space}}  با ابعاد بیشتر با تکنیک حقه‌ی هسته\footnote{\lr{Kernel trick}}  نگاشت می‌شود. در این فضا، ماشین بردار پشتیبان یک ابرصفحه جدا کننده بهینه برای جداسازی نمونه‌ها پیدا می‌کند. شکل \ref{fig:SVM-Ker} تکنیک حقه‌ی هسته را نشان می‌دهد.

\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.4]{SVM-Kernel}
	\caption{نگاشت نمونه‌ها با تکنیک حقه‌ی هسته}
	\label{fig:SVM-Ker}
\end{figure}

در نسخه غیر خطی ماشین بردار پشتیبان، مسئله \ref{eq:14} به صورت زیر تعریف می‌شود.
\begin{equation}
\begin{split} 
\min\limits_{\alpha} \quad &\frac{1}{2}\sum\limits_{i=1}^{m}{\sum\limits_{j=1}^{m}{{{\alpha }_{i}}{{\alpha }_{j}}{{y}_{i}}{{y}_{j}}K(x_{i},{{x}_{j}})-\sum\limits_{k=1}^{m}{{{\alpha }_{k}}}}} \\
\textrm{\lr{s.t. }} \quad &\sum\limits_{i=1}^{m}{\alpha_{i}y_{i}}=0, \\
&0 \le \alpha_{i}\le C,\text{ }\forall i
\end{split}
\label{eq:16}
\end{equation}
در رابطه \ref{eq:16}‏، تابع هسته  $K({{x}_{i}},{{x}_{j}})$ نمونه‌ها را به فضای ویژگی نگاشت می‌کند. تابع‌های هسته متداول شامل  \lr{RBF}\footnote{\lr{Radial basis function (RBF)}}، چند جمله‌ای و سیگموئید\footnote{\lr{Sigmoid}}  هستند. تابع تصمیم در نسخه غیر خطی به صورت زیر تعریف می‌شود.
\begin{equation}
D(x)= sign(\sum\limits_{i \in S}{\alpha_{i}y_{i}}K(x_{i}, x) + b)
\label{eq:17}
\end{equation}

\subsection{پیشینه پژوهش در ماشین بردار پشتیبان}\label{sec:2:1:4}
ماشین بردار پشتیبان بنیان ریاضی قوی‌ای دارد و دارای قدرت تعمیم‌پذیری بالایی است. از این رو در مسائل مختلف انند تشخیص آریتمی‌های قلبی \cite{nasiri2009}، شناسایی نفوذ به شبکه‌های کامپیوتری \cite{raman2017}، دسته‌بندی متن\cite{lee2012} و شناسایی 
هرزنامه \cite{zoubi2018} مورد استفاده قرار گرفته است. با این حال ماشین بردار پشتیبان نقاط ضعفی نیز دارد که مهم‌ترین آن‌ها عبارتند از:
\begin{enumerate}
	\item ماشین بردار پشتیبان برای بدست آوردن ابرصفحه بهینه یک مسئله بهینه‌سازی از نوع برنامه‌ریزی درجه دو حل می‌کند. مرتبه زمانی حل کردن چنین مسئله‌ای برابر با   $\mathcal{O}({{m}^{3}})$ است.  $m$ تعداد نمونه‌های آموزشی می‌باشد. این مرتبه زمانی، آموزش روش ماشین بردار پشتیبان را برای مجموعه داده‌های بزرگ به طور قابل توجه‌ای کند می‌کند.
	\item در روش \lr{SVM}، بردارهای پشتیبان نقش مهمی در ایجاد مدل دارند. در صورتی که این بردارها از نمونه‌های پرت یا نویزی باشد، دقت و تعمیم‌پذیری مدل خروجی کاهش می‌یابد. در نتیجه روش \lr{SVM} به نمونه‌های پرت و نویزی حساس است.
	\item چنانچه نمونه‌های یک کلاس از کلاس دیگر بسیار بیشتر باشد (مجموعه داده نامتوازن باشد.)، مدل ایجاد شده توسط  \lr{SVM} به سمت کلاس اکثریت گرایش پیدا می‌کند. در نتیجه مدل در تشخیص کلاس اقلیت ناتوان است.
\end{enumerate}
